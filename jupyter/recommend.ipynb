{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows: 155\nThat's weird - let's just drop them\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632677\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import pickle\n",
    "import csv\n",
    "import implicit\n",
    "import itertools\n",
    "import copy\n",
    "plt.style.use('ggplot')\n",
    "#read_csv()中的路径./指的是项目的根目录(即工作路径)\n",
    "df = pd.read_csv('./data/model_likes_anon.psv',\n",
    "                 sep='|', quoting=csv.QUOTE_MINIMAL,\n",
    "                 quotechar='\\\\')\n",
    "df.head()\n",
    "#获取表中重复记录的条数\n",
    "print('Duplicated rows: ' + str(df.duplicated().sum()))\n",
    "print('That\\'s weird - let\\'s just drop them')\n",
    "#删除重复的记录\n",
    "df.drop_duplicates(inplace=True)\n",
    "df = df[['uid', 'mid']]\n",
    "df.head()\n",
    "print df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting likes info\nNumber of users: 62583\nNumber of models: 28806\nSparsity: 0.035%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending likes info\nNumber of users: 15274\nNumber of models: 25655\nSparsity: 0.140%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "按条件过滤数据\n",
    "uid_min 过滤出用户查看次数 >=uid_min 的数据\n",
    "mid_min 过滤出物品被查看次数 >=mid_min 的数据\n",
    "'''\n",
    "def threshold_likes(df, uid_min, mid_min):\n",
    "    #获取有多少用户\n",
    "    n_users = df.uid.unique().shape[0]\n",
    "    #获取有多少item\n",
    "    n_items = df.mid.unique().shape[0]\n",
    "    #稀疏程度 值越小越稀疏\n",
    "    sparsity = float(df.shape[0]) / float(n_users*n_items) * 100\n",
    "    print('Starting likes info')\n",
    "    print('Number of users: {}'.format(n_users))\n",
    "    print('Number of models: {}'.format(n_items))\n",
    "    print('Sparsity: {:4.3f}%'.format(sparsity))\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        starting_shape = df.shape[0]\n",
    "        #按uid分组 得到的数据就是每个用户查看了多少次 其实按我的理解这里应该是uid_counts\n",
    "        uid_counts = df.groupby('uid').mid.count()\n",
    "        # 过滤掉用户查看次数小于mid_min的数据 ~：取非\n",
    "        df = df[~df.uid.isin(uid_counts[uid_counts < uid_min].index.tolist())]\n",
    "        #按mid分组 得到的数据就是每个item被查看了多少次 其实按我的理解这里应该是mid_counts\n",
    "        mid_counts = df.groupby('mid').uid.count()\n",
    "        # 过滤掉item被查看次数小于mid_min的数据\n",
    "        df = df[~df.mid.isin(mid_counts[mid_counts < mid_min].index.tolist())]\n",
    "        #循环直到再也不能过滤数据为止\n",
    "        ending_shape = df.shape[0]\n",
    "        if starting_shape == ending_shape:\n",
    "            done = True\n",
    "    #断言 所以这里的\n",
    "    assert(df.groupby('uid').mid.count().min() >= uid_min)\n",
    "    assert(df.groupby('mid').uid.count().min() >= mid_min)\n",
    "    \n",
    "    n_users = df.uid.unique().shape[0]\n",
    "    n_items = df.mid.unique().shape[0]\n",
    "    sparsity = float(df.shape[0]) / float(n_users*n_items) * 100\n",
    "    print('Ending likes info')\n",
    "    print('Number of users: {}'.format(n_users))\n",
    "    print('Number of models: {}'.format(n_items))\n",
    "    print('Sparsity: {:4.3f}%'.format(sparsity))\n",
    "    return df\n",
    "\n",
    "df_lim = threshold_likes(df,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping\n",
    "'''\n",
    "创建idx -> uid   idx -> item的映射\n",
    "idx由enumerate生产，从0开始 step为1  依次递增\n",
    "'''\n",
    "mid_to_idx = {}\n",
    "idx_to_mid = {}\n",
    "for (idx,mid) in enumerate(df_lim.mid.unique().tolist()):\n",
    "    mid_to_idx[mid] = idx\n",
    "    idx_to_mid[idx] = mid\n",
    "    \n",
    "uid_to_idx = {}\n",
    "idx_to_uid = {}\n",
    "for (idx, uid) in enumerate(df_lim.uid.unique().tolist()):\n",
    "    uid_to_idx[uid] = idx\n",
    "    idx_to_uid[idx] = uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15274, 25655)\n(15274, 25655)\n"
     ]
    }
   ],
   "source": [
    "def map_ids(row, mapper):\n",
    "    return mapper[row]\n",
    "# 获取uid对应的idx，返回成矩阵的形式，也就是numpy.ndarray\n",
    "I = df_lim.uid.apply(map_ids, args=[uid_to_idx]).as_matrix()\n",
    "# 获取item对应的idx，返回\n",
    "J = df_lim.mid.apply(map_ids, args=[mid_to_idx]).as_matrix()\n",
    "#根据uid的维度构建一个单位矩阵(在这里就是一个一维的向量)\n",
    "V = np.ones(I.shape[0])\n",
    "# 按坐标存储稀疏矩阵 I,J为坐标  V是值\n",
    "likes = sparse.coo_matrix((V, (I, J)), dtype=np.float64)\n",
    "print likes.shape\n",
    "# 按行压缩 比如同一个坐标出现了3次1 那么这个位置就是3\n",
    "likes = likes.tocsr()\n",
    "print likes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3054\n[    0     1     2 ..., 15265 15266 15267]\n"
     ]
    }
   ],
   "source": [
    "train = likes.copy().tocoo()\n",
    "print np.int32(np.floor(0.2 * train.shape[0]))\n",
    "print np.where(np.bincount(train.row) >= 5 * 2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "切分训练集和测试集\n",
    "'''\n",
    "def train_test_split(ratings, split_count, fraction=None):\n",
    "    \"\"\"\n",
    "    Split recommendation data into train and test sets\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    ratings : scipy.sparse matrix\n",
    "        Interactions between users and items.\n",
    "    split_count : int\n",
    "        Number of user-item-interactions per user to move\n",
    "        from training to test set.\n",
    "    fractions : float\n",
    "        Fraction of users to split off some of their\n",
    "        interactions into test set. If None, then all \n",
    "        users are considered.\n",
    "    \"\"\"\n",
    "    # Note: likely not the fastest way to do things below.\n",
    "    train = ratings.copy().tocoo()\n",
    "    test = sparse.lil_matrix(train.shape)\n",
    "    \n",
    "    if fraction:\n",
    "        try:\n",
    "            # 这里就是从8000多个用户中随机挑选出3054个用户 作为测试集\n",
    "            user_index = np.random.choice(\n",
    "                # 获取查看次数 >= split_count * 2 的用户ID的index，这里有8000多个用户\n",
    "                np.where(np.bincount(train.row) >= split_count * 2)[0], \n",
    "                replace=False,\n",
    "                # 总人数的0.2 = 3054\n",
    "                size=np.int32(np.floor(fraction * train.shape[0]))\n",
    "            ).tolist()\n",
    "        except:\n",
    "            print(('Not enough users with > {} '\n",
    "                  'interactions for fraction of {}')\\\n",
    "                  .format(2*k, fraction))\n",
    "            raise\n",
    "    else:\n",
    "        user_index = range(train.shape[0])\n",
    "        \n",
    "    train = train.tolil()\n",
    "\n",
    "    for user in user_index:\n",
    "        test_ratings = np.random.choice(ratings.getrow(user).indices, \n",
    "                                        size=split_count, \n",
    "                                        replace=False)\n",
    "        train[user, test_ratings] = 0.\n",
    "        # These are just 1.0 right now\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "   \n",
    "    # Test and training are truly disjoint\n",
    "    assert(train.multiply(test).nnz == 0)\n",
    "    return train.tocsr(), test.tocsr(), user_index\n",
    "train, test, user_index = train_test_split(likes, 2, fraction=0.2)\n",
    "print \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implicit Alternating Least Squares \"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "\n",
    "log = logging.getLogger(\"implicit\")\n",
    "\n",
    "\n",
    "class ALS(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_factors=40,\n",
    "                 regularization=0.01,\n",
    "                 alpha=1.0,\n",
    "                 iterations=15,\n",
    "                 use_native=True,\n",
    "                 num_threads=0,\n",
    "                 dtype=np.float64):\n",
    "        \"\"\"\n",
    "        Class version of alternating least squares implicit matrix factorization\n",
    "\n",
    "        Args:\n",
    "            num_factors (int): Number of factors to extract\n",
    "            regularization (double): Regularization parameter to use\n",
    "            iterations (int): Number of alternating least squares iterations to\n",
    "            run\n",
    "            use_native (bool): Whether or not to use Cython solver\n",
    "            num_threads (int): Number of threads to run least squares iterations.\n",
    "            0 means to use all CPU cores.\n",
    "            dtype (np dtype): Datatype for numpy arrays\n",
    "        \"\"\"\n",
    "        self.num_factors = num_factors\n",
    "        self.regularization = regularization\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.use_native = use_native\n",
    "        self.num_threads = num_threads\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit(self, Cui):\n",
    "        \"\"\"\n",
    "        Fit an alternating least squares model on Cui data\n",
    "\n",
    "        Args:\n",
    "            Cui (sparse matrix, shape=(num_users, num_items)): Matrix of\n",
    "            user-item \"interactions\"\n",
    "        \"\"\"\n",
    "\n",
    "        _check_open_blas()\n",
    "\n",
    "        users, items = Cui.shape\n",
    "\n",
    "        self.user_vectors = np.random.normal(size=(users, self.num_factors))\\\n",
    "                                     .astype(self.dtype)\n",
    "\n",
    "        self.item_vectors = np.random.normal(size=(items, self.num_factors))\\\n",
    "                                     .astype(self.dtype)\n",
    "\n",
    "        self.solver = implicit.als.least_squares if self.use_native else least_squares\n",
    "\n",
    "        self.fit_partial(Cui)\n",
    "\n",
    "    def fit_partial(self, Cui):\n",
    "        \"\"\"Continue fitting model\"\"\"\n",
    "\n",
    "        # Scaling\n",
    "        Cui = Cui.copy()\n",
    "        Cui.data *= self.alpha\n",
    "        Cui, Ciu = Cui.tocsr(), Cui.T.tocsr()\n",
    "\n",
    "        for iteration in range(self.iterations):\n",
    "            s = time.time()\n",
    "            self.solver(Cui,\n",
    "                        self.user_vectors,\n",
    "                        self.item_vectors,\n",
    "                        self.regularization,\n",
    "                        self.num_threads)\n",
    "            self.solver(Ciu,\n",
    "                        self.item_vectors,\n",
    "                        self.user_vectors,\n",
    "                        self.regularization,\n",
    "                        self.num_threads)\n",
    "            log.debug(\"finished iteration %i in %s\", iteration, time.time() - s)\n",
    "\n",
    "    def predict(self, user, item):\n",
    "        \"\"\"Predict for single user and item\"\"\"\n",
    "        return self.user_vectors[user, :].dot(self.item_vectors[item, :].T)\n",
    "\n",
    "    def predict_for_customers(self,):\n",
    "        \"\"\"Recommend products for all customers\"\"\"\n",
    "        return self.user_vectors.dot(self.item_vectors.T)\n",
    "\n",
    "    def predict_for_items(self, norm=True):\n",
    "        \"\"\"Recommend products for all products\"\"\"\n",
    "        pred = self.item_vectors.dot(self.item_vectors.T)\n",
    "        if norm:\n",
    "            norms = np.array([np.sqrt(np.diagonal(pred))])\n",
    "            pred = pred / norms / norms.T\n",
    "        return pred\n",
    "\n",
    "def alternating_least_squares(Cui, factors, regularization=0.01,\n",
    "                              iterations=15, use_native=True, num_threads=0,\n",
    "                              dtype=np.float64):\n",
    "    \"\"\" factorizes the matrix Cui using an implicit alternating least squares\n",
    "    algorithm\n",
    "\n",
    "    Args:\n",
    "        Cui (csr_matrix): Confidence Matrix\n",
    "        factors (int): Number of factors to extract\n",
    "        regularization (double): Regularization parameter to use\n",
    "        iterations (int): Number of alternating least squares iterations to\n",
    "        run\n",
    "        num_threads (int): Number of threads to run least squares iterations.\n",
    "        0 means to use all CPU cores.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (row, col) factors\n",
    "    \"\"\"\n",
    "    _check_open_blas()\n",
    "\n",
    "    users, items = Cui.shape\n",
    "\n",
    "    X = np.random.rand(users, factors).astype(dtype) * 0.01\n",
    "    Y = np.random.rand(items, factors).astype(dtype) * 0.01\n",
    "\n",
    "    Cui, Ciu = Cui.tocsr(), Cui.T.tocsr()\n",
    "\n",
    "    solver = implicit.als.least_squares if use_native else least_squares\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        s = time.time()\n",
    "        solver(Cui, X, Y, regularization, num_threads)\n",
    "        solver(Ciu, Y, X, regularization, num_threads)\n",
    "        log.debug(\"finished iteration %i in %s\", iteration, time.time() - s)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def least_squares(Cui, X, Y, regularization, num_threads):\n",
    "    \"\"\" For each user in Cui, calculate factors Xu for them\n",
    "    using least squares on Y.\n",
    "\n",
    "    Note: this is at least 10 times slower than the cython version included\n",
    "    here.\n",
    "    \"\"\"\n",
    "    users, factors = X.shape\n",
    "    YtY = Y.T.dot(Y)\n",
    "\n",
    "    for u in range(users):\n",
    "        # accumulate YtCuY + regularization*I in A\n",
    "        A = YtY + regularization * np.eye(factors)\n",
    "\n",
    "        # accumulate YtCuPu in b\n",
    "        b = np.zeros(factors)\n",
    "\n",
    "        for i, confidence in nonzeros(Cui, u):\n",
    "            factor = Y[i]\n",
    "            A += (confidence - 1) * np.outer(factor, factor)\n",
    "            b += confidence * factor\n",
    "\n",
    "        # Xu = (YtCuY + regularization * I)^-1 (YtCuPu)\n",
    "        X[u] = np.linalg.solve(A, b)\n",
    "\n",
    "\n",
    "def nonzeros(m, row):\n",
    "    \"\"\" returns the non zeroes of a row in csr_matrix \"\"\"\n",
    "    for index in range(m.indptr[row], m.indptr[row+1]):\n",
    "        yield m.indices[index], m.data[index]\n",
    "\n",
    "\n",
    "def _check_open_blas():\n",
    "    \"\"\" checks to see if using OpenBlas. If so, warn if the number of threads isn't set to 1\n",
    "    (causes perf issues) \"\"\"\n",
    "    if np.__config__.get_info('openblas_info') and os.environ.get('OPENBLAS_NUM_THREADS') != '1':\n",
    "        log.warn(\"OpenBLAS detected. Its highly recommend to set the environment variable \"\n",
    "                 \"'export OPENBLAS_NUM_THREADS=1' to disable its internal multithreading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def calculate_mse(model, ratings, user_index=None):\n",
    "    preds = model.predict_for_customers()\n",
    "    if user_index:\n",
    "        return mean_squared_error(ratings[user_index, :].toarray().ravel(),\n",
    "                                  preds[user_index, :].ravel())\n",
    "    \n",
    "    return mean_squared_error(ratings.toarray().ravel(),\n",
    "                              preds.ravel())\n",
    "\n",
    "def precision_at_k(model, ratings, k=5, user_index=None):\n",
    "    if not user_index:\n",
    "        user_index = range(ratings.shape[0])\n",
    "    ratings = ratings.tocsr()\n",
    "    precisions = []\n",
    "    # Note: line below may become infeasible for large datasets.\n",
    "    predictions = model.predict_for_customers()\n",
    "    for user in user_index:\n",
    "        # In case of large dataset, compute predictions row-by-row like below\n",
    "        # predictions = np.array([model.predict(row, i) for i in xrange(ratings.shape[1])])\n",
    "        top_k = np.argsort(-predictions[user, :])[:k]\n",
    "        labels = ratings.getrow(user).indices\n",
    "        precision = float(len(set(top_k) & set(labels))) / float(k)\n",
    "        precisions.append(precision)\n",
    "    return np.mean(precisions)  \n",
    "\n",
    "def print_log(row, header=False, spacing=12):\n",
    "    top = ''\n",
    "    middle = ''\n",
    "    bottom = ''\n",
    "    for r in row:\n",
    "        top += '+{}'.format('-'*spacing)\n",
    "        if isinstance(r, str):\n",
    "            middle += '| {0:^{1}} '.format(r, spacing-2)\n",
    "        elif isinstance(r, int):\n",
    "            middle += '| {0:^{1}} '.format(r, spacing-2)\n",
    "        elif isinstance(r, float):\n",
    "            middle += '| {0:^{1}.5f} '.format(r, spacing-2)\n",
    "        bottom += '+{}'.format('='*spacing)\n",
    "    top += '+'\n",
    "    middle += '|'\n",
    "    bottom += '+'\n",
    "    if header:\n",
    "        print(top)\n",
    "        print(middle)\n",
    "        print(bottom)\n",
    "    else:\n",
    "        print(middle)\n",
    "        print(top)\n",
    "        \n",
    "def learning_curve(model, train, test, epochs, k=5, user_index=None):\n",
    "    if not user_index:\n",
    "        user_index = range(train.shape[0])\n",
    "    prev_epoch = 0\n",
    "    train_precision = []\n",
    "    train_mse = []\n",
    "    test_precision = []\n",
    "    test_mse = []\n",
    "    \n",
    "    headers = ['epochs', 'p@k train', 'p@k test',\n",
    "               'mse train', 'mse test']\n",
    "    print_log(headers, header=True)\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        model.iterations = epoch - prev_epoch\n",
    "        if not hasattr(model, 'user_vectors'):\n",
    "            model.fit(train)\n",
    "        else:\n",
    "            model.fit_partial(train)\n",
    "        train_mse.append(calculate_mse(model, train, user_index))\n",
    "        train_precision.append(precision_at_k(model, train, k, user_index))\n",
    "        test_mse.append(calculate_mse(model, test, user_index))\n",
    "        test_precision.append(precision_at_k(model, test, k, user_index))\n",
    "        row = [epoch, train_precision[-1], test_precision[-1],\n",
    "               train_mse[-1], test_mse[-1]]\n",
    "        print_log(row)\n",
    "        prev_epoch = epoch\n",
    "    return model, train_precision, train_mse, test_precision, test_mse\n",
    "\n",
    "def grid_search_learning_curve(base_model, train, test, param_grid,\n",
    "                               user_index=None, patk=5, epochs=range(5, 25, 5)):\n",
    "    \"\"\"\n",
    "    \"Inspired\" (stolen) from sklearn gridsearch\n",
    "    https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_search.py\n",
    "    \"\"\"\n",
    "    curves = []\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    for v in itertools.product(*values):\n",
    "        params = dict(zip(keys, v))\n",
    "        this_model = copy.deepcopy(base_model)\n",
    "        print_line = []\n",
    "        for k, v in params.items():\n",
    "            setattr(this_model, k, v)\n",
    "            print_line.append((k, v))\n",
    "\n",
    "        print(' | '.join('{}: {}'.format(k, v) for (k, v) in print_line))\n",
    "        _, train_patk, train_mse, test_patk, test_mse = learning_curve(this_model, train, test,\n",
    "                                                                epochs, k=patk, user_index=user_index)\n",
    "        curves.append({'params': params,\n",
    "                       'patk': {'train': train_patk, 'test': test_patk},\n",
    "                       'mse': {'train': train_mse, 'test': test_mse}})\n",
    "    return curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1 | regularization: 0.0 | num_factors: 10\n+------------+------------+------------+------------+------------+\n|   epochs   | p@k train  |  p@k test  | mse train  |  mse test  |\n+============+============+============+============+============+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     5      |  0.13307   |  0.01146   |  0.00114   |  0.00027   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     10     |  0.12541   |  0.01081   |  0.00113   |  0.00027   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     15     |  0.12534   |  0.01107   |  0.00113   |  0.00027   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     20     |  0.12587   |  0.01113   |  0.00113   |  0.00027   |\n+------------+------------+------------+------------+------------+\nalpha: 1 | regularization: 0.0 | num_factors: 20\n+------------+------------+------------+------------+------------+\n|   epochs   | p@k train  |  p@k test  | mse train  |  mse test  |\n+============+============+============+============+============+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     5      |  0.16948   |  0.01421   |  0.00110   |  0.00031   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     10     |  0.16922   |  0.01382   |  0.00110   |  0.00031   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     15     |  0.16974   |  0.01401   |  0.00110   |  0.00031   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     20     |  0.17014   |  0.01395   |  0.00110   |  0.00031   |\n+------------+------------+------------+------------+------------+\nalpha: 1 | regularization: 0.0 | num_factors: 40\n+------------+------------+------------+------------+------------+\n|   epochs   | p@k train  |  p@k test  | mse train  |  mse test  |\n+============+============+============+============+============+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     5      |  0.24434   |  0.01611   |  0.00104   |  0.00036   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     10     |  0.24093   |  0.01526   |  0.00104   |  0.00037   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     15     |  0.24021   |  0.01513   |  0.00104   |  0.00037   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     20     |  0.24086   |  0.01532   |  0.00104   |  0.00036   |\n+------------+------------+------------+------------+------------+\nalpha: 1 | regularization: 0.0 | num_factors: 80\n+------------+------------+------------+------------+------------+\n|   epochs   | p@k train  |  p@k test  | mse train  |  mse test  |\n+============+============+============+============+============+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     5      |  0.35324   |  0.01618   |  0.00094   |  0.00047   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     10     |  0.35049   |  0.01735   |  0.00093   |  0.00047   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     15     |  0.34735   |  0.01716   |  0.00093   |  0.00047   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     20     |  0.34761   |  0.01690   |  0.00093   |  0.00047   |\n+------------+------------+------------+------------+------------+\nalpha: 1 | regularization: 0.1 | num_factors: 10\n+------------+------------+------------+------------+------------+\n|   epochs   | p@k train  |  p@k test  | mse train  |  mse test  |\n+============+============+============+============+============+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     5      |  0.12037   |  0.01048   |  0.00114   |  0.00027   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     10     |  0.12168   |  0.01035   |  0.00114   |  0.00027   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     15     |  0.12200   |  0.01087   |  0.00113   |  0.00027   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     20     |  0.12194   |  0.01074   |  0.00113   |  0.00027   |\n+------------+------------+------------+------------+------------+\nalpha: 1 | regularization: 0.1 | num_factors: 20\n+------------+------------+------------+------------+------------+\n|   epochs   | p@k train  |  p@k test  | mse train  |  mse test  |\n+============+============+============+============+============+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     5      |  0.17518   |  0.01395   |  0.00110   |  0.00030   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     10     |  0.17269   |  0.01388   |  0.00110   |  0.00031   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     15     |  0.17289   |  0.01395   |  0.00110   |  0.00031   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     20     |  0.17073   |  0.01362   |  0.00110   |  0.00031   |\n+------------+------------+------------+------------+------------+\nalpha: 1 | regularization: 0.1 | num_factors: 40\n+------------+------------+------------+------------+------------+\n|   epochs   | p@k train  |  p@k test  | mse train  |  mse test  |\n+============+============+============+============+============+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     5      |  0.24892   |  0.01585   |  0.00104   |  0.00036   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     10     |  0.24407   |  0.01539   |  0.00104   |  0.00036   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     15     |  0.24185   |  0.01480   |  0.00104   |  0.00036   |\n+------------+------------+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     20     |  0.24198   |  0.01493   |  0.00104   |  0.00036   |\n+------------+------------+------------+------------+------------+\nalpha: 1 | regularization: 0.1 | num_factors: 80\n+------------+------------+------------+------------+------------+\n|   epochs   | p@k train  |  p@k test  | mse train  |  mse test  |\n+============+============+============+============+============+\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'num_factors': [10, 20, 40],\n",
    "              'regularization': [0.0, 1e-1, 1e1],\n",
    "              'alpha': [1, 10, 50]}\n",
    "\n",
    "base_model = ALS()\n",
    "\n",
    "curves = grid_search_learning_curve(base_model, train, test,\n",
    "                                    param_grid,\n",
    "                                    user_index=user_index,\n",
    "                                    patk=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'curves' is not defined",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-625af8050b71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_curves\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurves\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'patk'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'curves' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "best_curves = sorted(curves, key=lambda x: max(x['patk']['test']), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}